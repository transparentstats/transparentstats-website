---
title: "Statements for the 2021 VIS Committee Elections"
date: "2016-01-01"
---

A new member is being elected to the VIS Steering Committee (VSC) and the VIS Executive Committee (VEC), which provide scientific and organizational oversight of the IEEE VIS conference and reviewing process.

Following [the example set by Society for the Improvement of Psychological Science](https://improvingpsych.org/mission/election-statements/), members of the Transparent Statistics in HCI asked 2021 VSC and VEC candidates to publicly answer a question about research transparency and open practices: 

> **If elected to the IEEE VIS [Committee], what (if any) policies would you promote to improve research in visualization, and how would you support open science practices and research transparency at IEEE VIS and in the field of visualization more broadly?**

Because different areas within visualization vary considerably in existing norms and practices, and because the specific issues that are most salient may vary by sub-discipline, the question is very broad and open-ended. The email was signed by Steve Haroz, Fanny Chevalier, Lewis Chuang, Pierre Dragicevic, Shion Guha, and Matthew Kay.

Statements will be posted as they are received.

# VIS Steering Committee

### [Chris Johnson](http://ieeevis.org/year/2021/info/vsc-candidates#chris-johnson)

*Statement not yet received (request sent Sept. 13)*

### [Kwan-Liu Ma](http://ieeevis.org/year/2021/info/vsc-candidates#kwan-li-uma)

Even though VIS has created a set of well thought through guidelines for both authors and reviewers, we should not stop improving the paper review and selection process since this process is so crucial to the advancement of the field.   I found the overemphasis of thorough evaluation and user studies for TVCG track papers often leads to acceptance of  papers making only incremental contributions while rejecting papers presenting novel ideas. I like how other scientific disciplines recognize conference presentations focusing on original ideas and consider archiving subsequently.  I would like to further promote the short papers or bring back a highly recognized conference track program.  When asked to serve on this year’s VIS short papers committee, I was glad to find the emphasis of new and novel contributions rather than treating short papers as second class. So it’s a good start but not enough.  I will also propose to re-examine the current VIS paper reviewing process for further improving the quality of the reviews and selections.

I am fully behind open science practices and research transparency.   However, we need to be careful and reasonable in setting the expectation and educating both the researchers and reviewers about open science and  reproducibility.   All these require a community effort, and VIS/VGTC should lead and sponsor this effort. Therefore, I would support organizing and offering regular VIS conference workshops/tutorials  as well as the development of online instruction materials on research transparency. Compared to 10 years ago, we have more basis and resources to teach visualization, but what is considered comprehensive education for visualization researchers?  As a field, we should collectively develop a curriculum that includes transparent research in VIS.

### [Bernhard Preim](http://ieeevis.org/year/2021/info/vsc-candidates#bernhard-preim)

I consider open science, e.g., open source software and open data, as essential primarily to foster reproducibility. The fact that many scientific results cannot be reproduced, is a severe problem for science in general. Thus, I consider the Graphics Replicability Stamp Initiative as an essential step in this direction and would support further initiatives to strengthen transparency. However, I am also aware of differences in the subcommunities. While open source software and public databases play an essential role in BioVis and BioInformatics, in medical applications, it is still rare that data can be made publicly available. Without example datasets, even an open source software is of limitied value. Thus, I would not favor that open source software is mandatory for publications at Vis.

### [Anna Vilanova](http://ieeevis.org/year/2021/info/vsc-candidates#anna-vilanova)

*Statement not yet received (request sent Sept. 13)*

# VIS Executive Committee

### [Rita Borgo](http://ieeevis.org/year/2021/info/vec-candidates#rita-borgo)

*Statement not yet received (request sent Sept. 17)*

### [Stefan Bruckner](http://ieeevis.org/year/2021/info/vec-candidates#stefan-bruckner)

*Statement not yet received (request sent Sept. 17)*

### [Michael Correll](http://ieeevis.org/year/2021/info/vec-candidates#michael-correll)

As a researcher in communicating uncertainty and statistical concepts to non-statistical audiences, as a participant in organizations dedicated to statistical openness (such as the Transparent Statistics in HCI group), and as a frequent commenter on the epistemological and communicative conundrums at the core of visualization research, establishing thoughtful and rigorous norms around how we conduct and report on our research is a core concern of mine. Hopefully the centrality of this concern is reflected in both my work and my VEC candidate statement. I will use my response here to highlight some potential areas where I think we could (or should) change as a conference.

I will preface any statements about policy by saying that one of the things that draws me to the visualization community is that we come from many backgrounds and have a wide and varied view of what it means to do "good" visualization research. Any schemes to "improve" research in visualization must be undertaken from this assumption of plurality: standards that work for an author presenting a graphical perception study would look quite different than standards for an author presenting a novel rendering algorithm, or one presenting a close reading of a visualization on historical or aesthetic grounds.

With that said, I believe that we can look to other fields and conferences for examples of policies that work to encourage openness, rigor, and reliability. Both the disruptions to the usual way of presenting work caused by the pandemic and by new legislation such as the EU's Plan S are long-overdue crises that will require us to rethink the publication model in any event, so why not build one that is more conducive to better practices from the outset? "Encourage" is the key word for me here; I think we will have more success providing positive reinforcement for high-quality research as opposed to introducing new, stricter, and potentially unfamiliar rules or punishments. Other conferences are taking big bets or performing big experiments around how work is submitted (rolling deadlines, revise-and-resubmit options, and rebuttals) as well as on the contents and structure of that submitted work (requiring statements of broader impacts, positionally, or data accessibility). As we learn more about the results of those experiments, we should borrow, adapt, or outright copy the things that work. 

One particular form of encouragement I would like to see is a more expansive view of conference contributions other than "10 pages of static content due on a specific date and published in isolation in a special edition of a journal." Preregistrations, registered reports, and replications are artifacts that can shore up the rigor of empirical work. Yet, these artifacts are currently rare or, at best, co-exist uneasily with the standard IEEE VIS reviewing and publication model. We need to make room for this sort of work at VIS. Providing official and explicit guidance, recognition, and promotion for these sorts of "non-standard" research artifacts would go a long way towards moving these practices from the periphery to the core of empirical work at VIS.

### [Helwig Hauser](http://ieeevis.org/year/2021/info/vec-candidates#helwig-hauser)

*Statement not yet received (request sent Sept. 17)*

### [Filip Sadlo](http://ieeevis.org/year/2021/info/vec-candidates#filip-sadlo)

*Statement not yet received (request sent Sept. 17)*
